{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDmt8Rj76nLg"
      },
      "source": [
        "# Project Streamlit\n",
        "\n",
        "- modeling the Titanic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD5OR_mc6pxG"
      },
      "source": [
        "- Course Name :         Applied Machine Learning\n",
        "- Course instructor:    Sohail Tehranipour\n",
        "- Student Name :        Afshin Masoudi Ashtiani\n",
        "- Chapter 7 -           Building a Web App for Data Scientists\n",
        "- Project:              Streamlit Project\n",
        "- Date :                September 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfXb2ejuDDTa"
      },
      "source": [
        "## Step 1: Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Figuxc0W52eF",
        "outputId": "a7ed1286-02a2-47a4-e2a3-f380eb712a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: xgboost in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from pandas) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\afshin\\desktop\\titanic-streamlit_app\\streamlit_app_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas scikit-learn xgboost joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0duj1B80DvP6"
      },
      "source": [
        "## Step 2: Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "NcyMUXShDwxD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tabulate import tabulate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, cohen_kappa_score, matthews_corrcoef\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H1v0a0PGLCU"
      },
      "source": [
        "## Step 3: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = r'./repository/train.csv'\n",
        "display = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "luyO2cDVGRHz",
        "outputId": "4a606400-b23d-4da8-a52f-c94679977402"
      },
      "outputs": [],
      "source": [
        "# \"\"\"Load the dataset from a CSV file from google.drive\"\"\"\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# df = pd.read_csv('/content/drive/My Drive/Applied Machine Learning/Datasets/titanic_train.csv')\n",
        "# X = df.drop(labels='Survived', axis=1)\n",
        "# y = df.Survived"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "TVf0U7xDG4yZ"
      },
      "outputs": [],
      "source": [
        "\"\"\"Load the dataset from a CSV file.\"\"\"\n",
        "df = pd.read_csv(train_path)\n",
        "X = df.drop(labels='Survived', axis=1)\n",
        "y = df.Survived"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---------------+------------+----------+-----------------------------------------------------+--------+-------+---------+---------+------------------+---------+---------+------------+\n",
            "|    |   PassengerId |   Survived |   Pclass | Name                                                | Sex    |   Age |   SibSp |   Parch | Ticket           |    Fare | Cabin   | Embarked   |\n",
            "|----+---------------+------------+----------+-----------------------------------------------------+--------+-------+---------+---------+------------------+---------+---------+------------|\n",
            "|  0 |             1 |          0 |        3 | Braund, Mr. Owen Harris                             | male   |    22 |       1 |       0 | A/5 21171        |  7.25   | nan     | S          |\n",
            "|  1 |             2 |          1 |        1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female |    38 |       1 |       0 | PC 17599         | 71.2833 | C85     | C          |\n",
            "|  2 |             3 |          1 |        3 | Heikkinen, Miss. Laina                              | female |    26 |       0 |       0 | STON/O2. 3101282 |  7.925  | nan     | S          |\n",
            "|  3 |             4 |          1 |        1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female |    35 |       1 |       0 | 113803           | 53.1    | C123    | S          |\n",
            "|  4 |             5 |          0 |        3 | Allen, Mr. William Henry                            | male   |    35 |       0 |       0 | 373450           |  8.05   | nan     | S          |\n",
            "|  5 |             6 |          0 |        3 | Moran, Mr. James                                    | male   |   nan |       0 |       0 | 330877           |  8.4583 | nan     | Q          |\n",
            "|  6 |             7 |          0 |        1 | McCarthy, Mr. Timothy J                             | male   |    54 |       0 |       0 | 17463            | 51.8625 | E46     | S          |\n",
            "|  7 |             8 |          0 |        3 | Palsson, Master. Gosta Leonard                      | male   |     2 |       3 |       1 | 349909           | 21.075  | nan     | S          |\n",
            "|  8 |             9 |          1 |        3 | Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   | female |    27 |       0 |       2 | 347742           | 11.1333 | nan     | S          |\n",
            "|  9 |            10 |          1 |        2 | Nasser, Mrs. Nicholas (Adele Achem)                 | female |    14 |       1 |       0 | 237736           | 30.0708 | nan     | C          |\n",
            "+----+---------------+------------+----------+-----------------------------------------------------+--------+-------+---------+---------+------------------+---------+---------+------------+\n"
          ]
        }
      ],
      "source": [
        "print(tabulate(df[:10], headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Split the dataset into Training and Testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kQlNCNIvHEwh",
        "outputId": "159c496d-1106-42e8-af7a-af4498a8c89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+---------------+----------+-----------------------------------------------------+--------+-------+---------+---------+---------------+----------+---------+------------+\n",
            "|     |   PassengerId |   Pclass | Name                                                | Sex    |   Age |   SibSp |   Parch | Ticket        |     Fare | Cabin   | Embarked   |\n",
            "|-----+---------------+----------+-----------------------------------------------------+--------+-------+---------+---------+---------------+----------+---------+------------|\n",
            "| 677 |           678 |        3 | Turja, Miss. Anna Sofia                             | female |    18 |       0 |       0 | 4138          |   9.8417 | nan     | S          |\n",
            "| 547 |           548 |        2 | Padro y Manent, Mr. Julian                          | male   |   nan |       0 |       0 | SC/PARIS 2146 |  13.8625 | nan     | C          |\n",
            "| 317 |           318 |        2 | Moraweck, Dr. Ernest                                | male   |    54 |       0 |       0 | 29011         |  14      | nan     | S          |\n",
            "| 261 |           262 |        3 | Asplund, Master. Edvin Rojj Felix                   | male   |     3 |       4 |       2 | 347077        |  31.3875 | nan     | S          |\n",
            "| 273 |           274 |        1 | Natsch, Mr. Charles H                               | male   |    37 |       0 |       1 | PC 17596      |  29.7    | C118    | C          |\n",
            "| 715 |           716 |        3 | Soholt, Mr. Peter Andreas Lauritz Andersen          | male   |    19 |       0 |       0 | 348124        |   7.65   | F G73   | S          |\n",
            "| 310 |           311 |        1 | Hays, Miss. Margaret Bechstein                      | female |    24 |       0 |       0 | 11767         |  83.1583 | C54     | C          |\n",
            "| 269 |           270 |        1 | Bissette, Miss. Amelia                              | female |    35 |       0 |       0 | PC 17760      | 135.633  | C99     | S          |\n",
            "| 103 |           104 |        3 | Johansson, Mr. Gustaf Joel                          | male   |    33 |       0 |       0 | 7540          |   8.6542 | nan     | S          |\n",
            "| 600 |           601 |        2 | Jacobsohn, Mrs. Sidney Samuel (Amy Frances Christy) | female |    24 |       2 |       1 | 243847        |  27      | nan     | S          |\n",
            "+-----+---------------+----------+-----------------------------------------------------+--------+-------+---------+---------+---------------+----------+---------+------------+\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.1, random_state= 123)\n",
        "print(tabulate(X_train[:10], headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2gTp_nKHv0h"
      },
      "source": [
        "## Step 5: Pre-Process the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create initial pre-process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "9XOs3j93H2xf"
      },
      "outputs": [],
      "source": [
        "class PreProcessor(BaseEstimator, TransformerMixin): \n",
        "    def fit(self, X, y=None): \n",
        "        self.ageImputer = SimpleImputer()\n",
        "        self.ageImputer.fit(X[['Age']])        \n",
        "        return self \n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        X['Age'] = self.ageImputer.transform(X[['Age']])\n",
        "        X['CabinClass'] = X['Cabin'].fillna('M').apply(lambda x: str(x).replace(' ', '')).apply(lambda x: re.sub(r'[^a-zA-Z]', '', x))\n",
        "        X['CabinNumber'] = X['Cabin'].fillna('M').apply(lambda x: str(x).replace(' ', '')).apply(lambda x: re.sub(r'[^0-9]', '', x)).replace('', 0) \n",
        "        X['Embarked'] = X['Embarked'].fillna('M')\n",
        "        X = X.drop(['PassengerId', 'Name', 'Ticket','Cabin'], axis=1)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_pipeline(pipeline:Pipeline, name:str) -> str:\n",
        "    if pipeline:\n",
        "        file_name = f'{(''.join(cap for cap in str(name) if cap.isupper())).lower()}pipe.joblib'\n",
        "        joblib.dump(value= pipeline, filename= file_name) \n",
        "        return file_name   \n",
        "    else:\n",
        "        print(\"> No pipeline found to save ...!\")\n",
        "        return ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create pipelines for training models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create columns transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessor = PreProcessor()\n",
        "numeric_pipeline = Pipeline([('Scaler', StandardScaler())])\n",
        "categorical_pipeline = Pipeline([('OneHot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "transformer = ColumnTransformer([('num', numeric_pipeline, ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'CabinNumber']), ('cat', categorical_pipeline, ['Sex', 'Embarked', 'CabinClass'])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Create pipelines and fit the pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "3ipvU2Tzo-Bt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+---------------------+-----------------+\n",
            "|    | Model               | Filename        |\n",
            "|----+---------------------+-----------------|\n",
            "|  0 | LogisticRegression  | lrpipe.joblib   |\n",
            "|  1 | RidgeClassifier     | rcpipe.joblib   |\n",
            "|  2 | ExtraTreeClassifier | etcpipe.joblib  |\n",
            "|  3 | XGBClassifier       | xgbcpipe.joblib |\n",
            "+----+---------------------+-----------------+\n"
          ]
        }
      ],
      "source": [
        "lrpipe = Pipeline([('InitialPreProc', preprocessor), ('Transformer', transformer), ('Logistic Regression', LogisticRegression())]).fit(X_train, y_train)\n",
        "rcpipe = Pipeline([('InitialPreProc', preprocessor), ('Transformer', transformer), ('Ridge Classifier', RidgeClassifier())]).fit(X_train, y_train)\n",
        "etcpipe = Pipeline([('InitialPreProc', preprocessor), ('Transformer', transformer), ('Extra Tree Classifier', ExtraTreeClassifier())]).fit(X_train, y_train)\n",
        "xgbcpipe = Pipeline([('InitialPreProc', preprocessor), ('Transformer', transformer), ('XGB Classifier', XGBClassifier())]).fit(X_train, y_train)\n",
        "\n",
        "pipelines_df = pd.DataFrame([\n",
        "    {'Model' : 'LogisticRegression', 'Filename' : save_pipeline(lrpipe, 'LogisticRegression')}, \n",
        "    {'Model' : 'RidgeClassifier', 'Filename' : save_pipeline(rcpipe, 'RidgeClassifier')}, \n",
        "    {'Model' : 'ExtraTreeClassifier', 'Filename' : save_pipeline(etcpipe, 'ExtraTreeClassifier')}, \n",
        "    {'Model' : 'XGBClassifier', 'Filename' : save_pipeline(xgbcpipe, 'XGBClassifier')}])\n",
        "\n",
        "print(tabulate(pipelines_df, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Evaluate the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+---------------------+------------+----------+----------+----------+----------+----------+----------+------------+\n",
            "|      | Model               |   Accuracy |      AUC |   Recall |    Prec. |       F1 |    Kappa |      MCC |   TT (Sec) |\n",
            "|------+---------------------+------------+----------+----------+----------+----------+----------+----------+------------|\n",
            "| lr   | LogisticRegression  |   0.811111 | 0.823733 | 0.857143 | 0.648649 | 0.738462 | 0.595024 | 0.60919  |    0.0625  |\n",
            "| rc   | RidgeClassifier     |   0.811111 | 0.804147 | 0.785714 | 0.666667 | 0.721311 | 0.579901 | 0.584379 |    0.3125  |\n",
            "| etc  | ExtraTreeClassifier |   0.766667 | 0.762097 | 0.75     | 0.6      | 0.666667 | 0.490566 | 0.497796 |    0.03125 |\n",
            "| xgbc | XGBClassifier       |   0.833333 | 0.810484 | 0.75     | 0.724138 | 0.736842 | 0.614946 | 0.615149 |    0.0625  |\n",
            "+------+---------------------+------------+----------+----------+----------+----------+----------+----------+------------+\n"
          ]
        }
      ],
      "source": [
        "eval_list = []\n",
        "index_list = []\n",
        "\n",
        "start_time = time.process_time()\n",
        "for index, row in pipelines_df.iterrows():\n",
        "    model = joblib.load(filename= row.Filename)\n",
        "    y_pred = model.predict(X_test)\n",
        "    current_time = time.process_time()\n",
        "            \n",
        "    eval_list.append({\n",
        "        'Model' : row.Model, \n",
        "        'Accuracy' : accuracy_score(y_test, y_pred), \n",
        "        'AUC' : roc_auc_score(y_test, y_pred), \n",
        "        'Recall' : recall_score(y_test, y_pred), \n",
        "        'Prec.' : precision_score(y_test, y_pred), \n",
        "        'F1' : f1_score(y_test, y_pred),\n",
        "        'Kappa' : cohen_kappa_score(y_test, y_pred),\n",
        "        'MCC' : matthews_corrcoef(y_test, y_pred),\n",
        "        'TT (Sec)' : current_time - start_time})\n",
        "    \n",
        "    index_list.append((''.join(cap for cap in str(row.Model) if cap.isupper())).lower())\n",
        "\n",
        "    start_time = current_time\n",
        "\n",
        "eval_df = pd.DataFrame(eval_list, index= index_list)\n",
        "print(tabulate(eval_df, headers='keys', tablefmt='psql'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Find the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> The best model is XGBClassifier.\n"
          ]
        }
      ],
      "source": [
        "best_index = eval_df.Accuracy.idxmax()\n",
        "best_name = eval_df.loc[best_index, 'Model']\n",
        "best_path = f'{best_index}pipe.joblib'\n",
        "best_model = joblib.load(best_path)\n",
        "print(f'>> The best model is {best_name}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Save the best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "FEaLAMvQae19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['xgbcpipe.joblib']"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joblib.dump(value= best_model, filename= best_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Predict the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Import the saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "7CQVLX0Sasio"
      },
      "outputs": [],
      "source": [
        "model = joblib.load(filename='lrpipe.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Import the testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  418 non-null    int64  \n",
            " 1   Pclass       418 non-null    int64  \n",
            " 2   Name         418 non-null    object \n",
            " 3   Sex          418 non-null    object \n",
            " 4   Age          332 non-null    float64\n",
            " 5   SibSp        418 non-null    int64  \n",
            " 6   Parch        418 non-null    int64  \n",
            " 7   Ticket       418 non-null    object \n",
            " 8   Fare         417 non-null    float64\n",
            " 9   Cabin        91 non-null     object \n",
            " 10  Embarked     418 non-null    object \n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 36.1+ KB\n"
          ]
        }
      ],
      "source": [
        "test = pd.read_csv(r'./repository/test.csv')\n",
        "test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Predict the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------------+\n",
            "|    |   Prediction |\n",
            "|----+--------------|\n",
            "|  0 |            0 |\n",
            "|  1 |            1 |\n",
            "|  2 |            1 |\n",
            "|  3 |            1 |\n",
            "|  4 |            0 |\n",
            "|  5 |            0 |\n",
            "|  6 |            0 |\n",
            "|  7 |            0 |\n",
            "|  8 |            1 |\n",
            "|  9 |            1 |\n",
            "+----+--------------+\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(df)\n",
        "y_pred_df = pd.DataFrame(y_pred, columns= ['Prediction'])\n",
        "print(tabulate(y_pred_df[:10], headers='keys', tablefmt='psql'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
